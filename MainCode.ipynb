{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9K3yfC+khgTxLjJQy3z1y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namankansal000/AutoJudge/blob/main/MainCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em0e32Uhyi7H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_json('/content/problems_data.jsonl', lines=True)\n",
        "\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df['problem_class'].value_counts())\n",
        "print(\"\\nScore statistics:\")\n",
        "print(df['problem_score'].describe())\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "df['problem_class'].value_counts().plot(kind='bar')\n",
        "plt.title('Class Distribution')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "df['problem_score'].hist(bins=30)\n",
        "plt.title('Score Distribution')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------\n",
        "# 1Ô∏è‚É£ Imports\n",
        "# ------------------------------\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Download stopwords (first time only)\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# ------------------------------\n",
        "# 2Ô∏è‚É£ Text preprocessing functions\n",
        "# ------------------------------\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean text and remove stopwords\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = ' '.join([w for w in text.split() if w not in stop_words])\n",
        "    return text\n",
        "\n",
        "def combine_text_fields(row):\n",
        "    \"\"\"Combine all text fields into one\"\"\"\n",
        "    combined = f\"{row['title']} {row['description']} {row['input_description']} {row['output_description']}\"\n",
        "    return preprocess_text(combined)\n",
        "\n",
        "# ------------------------------\n",
        "# 3Ô∏è‚É£ Apply preprocessing to dataframe\n",
        "# ------------------------------\n",
        "df['combined_text'] = df.apply(combine_text_fields, axis=1)\n",
        "df['text_length'] = df['combined_text'].apply(len)\n",
        "\n",
        "# Map class labels to numbers\n",
        "class_mapping = {'hard': 0, 'medium': 1, 'easy': 2}\n",
        "df['class_label'] = df['problem_class'].map(class_mapping)\n",
        "\n",
        "print(f\"Processed {len(df)} samples\")\n",
        "print(f\"Average text length: {df['text_length'].mean():.0f} characters\")\n",
        "print(\"Class distribution before split:\")\n",
        "print(df['class_label'].value_counts())\n",
        "\n",
        "# ------------------------------\n",
        "# 4Ô∏è‚É£ Convert text to numeric features (TF-IDF)\n",
        "# ------------------------------\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # limit vocab size\n",
        "X = vectorizer.fit_transform(df['combined_text'])\n",
        "y = df['class_label']\n",
        "\n",
        "# ------------------------------\n",
        "# 5Ô∏è‚É£ Train/test split (stratified)\n",
        "# ------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\nClass distribution in training set (before SMOTE):\")\n",
        "print(Counter(y_train))\n",
        "\n",
        "# ------------------------------\n",
        "# 6Ô∏è‚É£ Handle class imbalance with SMOTE\n",
        "# ------------------------------\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nClass distribution in training set (after SMOTE):\")\n",
        "print(Counter(y_train_res))\n",
        "\n",
        "# ------------------------------\n",
        "# ‚úÖ Now X_train_res, y_train_res and X_test, y_test are ready for training classifiers\n",
        "# ------------------------------\n"
      ],
      "metadata": {
        "id": "oKDfi4DdysB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------\n",
        "# 1Ô∏è‚É£ TF-IDF features\n",
        "# ------------------------------\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,   # Limit number of features\n",
        "    min_df=5,             # Ignore words appearing in <5 documents\n",
        "    max_df=0.7,           # Ignore words appearing in >70% documents\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2)    # Unigrams + Bigrams\n",
        ")\n",
        "\n",
        "X_tfidf = tfidf.fit_transform(df['combined_text'])\n",
        "\n",
        "# ------------------------------\n",
        "# 2Ô∏è‚É£ Scale numeric features\n",
        "# ------------------------------\n",
        "scaler = StandardScaler()\n",
        "text_len_scaled = scaler.fit_transform(df['text_length'].values.reshape(-1, 1))\n",
        "\n",
        "# Combine TF-IDF + numeric features\n",
        "X_features = hstack([X_tfidf, csr_matrix(text_len_scaled)])\n",
        "\n",
        "# ------------------------------\n",
        "# 3Ô∏è‚É£ Prepare labels\n",
        "# ------------------------------\n",
        "y_class = df['class_label'].values\n",
        "y_score = df['problem_score'].values  # if you also want regression\n",
        "\n",
        "# ------------------------------\n",
        "# 4Ô∏è‚É£ Train/test split (stratified)\n",
        "# ------------------------------\n",
        "X_train, X_test, y_class_train, y_class_test = train_test_split(\n",
        "    X_features, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "print(\"Class distribution in training set (before SMOTE):\", Counter(y_class_train))\n",
        "\n",
        "# ------------------------------\n",
        "# 5Ô∏è‚É£ Handle class imbalance with SMOTE\n",
        "# ------------------------------\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_class_train_res = smote.fit_resample(X_train, y_class_train)\n",
        "\n",
        "print(\"Class distribution in training set (after SMOTE):\", Counter(y_class_train_res))\n",
        "\n",
        "# ------------------------------\n",
        "# ‚úÖ Now ready for classifier training\n",
        "# X_train_res, y_class_train_res -> train\n",
        "# X_test, y_class_test -> test\n",
        "# ------------------------------\n",
        "print(f\"Training samples: {X_train_res.shape[0]}\")\n",
        "print(f\"Test samples: {X_test.shape[0]}\")\n",
        "print(f\"Number of features: {X_train_res.shape[1]}\")\n"
      ],
      "metadata": {
        "id": "bd_xU2aDywZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Train different classifiers\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=3000, random_state=42,class_weight='balanced'),\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=30,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        ",\n",
        "    'svm' : LinearSVC(class_weight='balanced',max_iter=5000)\n",
        "\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    clf.fit(X_train, y_class_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Evaluation\n",
        "    accuracy = accuracy_score(y_class_test, y_pred)\n",
        "    results[name] = {\n",
        "        'model': clf,\n",
        "        'accuracy': accuracy\n",
        "    }\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_class_test, y_pred,\n",
        "                                target_names=['hard', 'medium', 'easy']))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_class_test, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "# Select best classifier\n",
        "best_clf_name = max(results, key=lambda x: results[x]['accuracy'])\n",
        "best_clf = results[best_clf_name]['model']\n",
        "print(f\"\\nBest classifier: {best_clf_name} with accuracy: {results[best_clf_name]['accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "Ds_IW2EgyyyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Ensure the 'models' directory exists\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "X_train, X_test, y_class_train, y_class_test, y_score_train, y_score_test = train_test_split(\n",
        "    X_features,\n",
        "    y_class,\n",
        "    y_score,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_class\n",
        ")\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_score_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "mae_lr = mean_absolute_error(y_score_test, y_pred_lr)\n",
        "rmse_lr = np.sqrt(mean_squared_error(y_score_test, y_pred_lr))\n",
        "\n",
        "print(\"Linear Regression\")\n",
        "print(f\"MAE: {mae_lr:.3f}\")\n",
        "print(f\"RMSE: {rmse_lr:.3f}\")\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=30,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_reg.fit(X_train, y_score_train)\n",
        "\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "mae_rf = mean_absolute_error(y_score_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_score_test, y_pred_rf))\n",
        "\n",
        "print(\"\\nRandom Forest Regressor\")\n",
        "print(f\"MAE: {mae_rf:.3f}\")\n",
        "print(f\"RMSE: {rmse_rf:.3f}\")\n",
        "gbr = GradientBoostingRegressor(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gbr.fit(X_train, y_score_train)\n",
        "\n",
        "y_pred_gb = gbr.predict(X_test)\n",
        "\n",
        "mae_gb = mean_absolute_error(y_score_test, y_pred_gb)\n",
        "rmse_gb = np.sqrt(mean_squared_error(y_score_test, y_pred_gb))\n",
        "\n",
        "print(\"\\nGradient Boosting Regressor\")\n",
        "print(f\"MAE: {mae_gb:.3f}\")\n",
        "print(f\"RMSE: {rmse_gb:.3f}\")\n",
        "results_reg = {\n",
        "    \"Linear Regression\": mae_lr,\n",
        "    \"Random Forest\": mae_rf,\n",
        "    \"Gradient Boosting\": mae_gb\n",
        "}\n",
        "\n",
        "best_reg_name = min(results_reg, key=results_reg.get)\n",
        "print(f\"\\nBest Regression Model: {best_reg_name}\")\n",
        "best_regressor = {\n",
        "    \"Linear Regression\": lr,\n",
        "    \"Random Forest\": rf_reg,\n",
        "    \"Gradient Boosting\": gbr\n",
        "}[best_reg_name]\n",
        "\n",
        "joblib.dump(best_regressor, \"models/regressor.pkl\")"
      ],
      "metadata": {
        "id": "Lo0HvwvRy1I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "joblib.dump(best_clf, \"models/classifier.pkl\")\n",
        "joblib.dump(best_regressor, \"models/regressor.pkl\")\n",
        "joblib.dump(tfidf, \"models/tfidf.pkl\")\n",
        "joblib.dump(scaler, \"models/scaler.pkl\")\n"
      ],
      "metadata": {
        "id": "XXgehO_By4cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import re\n",
        "import numpy as np\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# -------------------------\n",
        "# Load Models\n",
        "# -------------------------\n",
        "classifier = joblib.load(\"models/classifier.pkl\")\n",
        "regressor = joblib.load(\"models/regressor.pkl\")\n",
        "tfidf = joblib.load(\"models/tfidf.pkl\")\n",
        "scaler = joblib.load(\"models/scaler.pkl\")\n",
        "\n",
        "# -------------------------\n",
        "# Text Preprocessing\n",
        "# -------------------------\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# -------------------------\n",
        "# Streamlit UI\n",
        "# -------------------------\n",
        "st.set_page_config(page_title=\"AutoJudge\", layout=\"centered\")\n",
        "\n",
        "st.title(\"ü§ñ AutoJudge\")\n",
        "st.subheader(\"Predict Programming Problem Difficulty\")\n",
        "\n",
        "st.markdown(\"Enter the problem details below:\")\n",
        "\n",
        "title = st.text_input(\"Problem Title\")\n",
        "description = st.text_area(\"Problem Description\", height=200)\n",
        "input_desc = st.text_area(\"Input Description\", height=150)\n",
        "output_desc = st.text_area(\"Output Description\", height=150)\n",
        "\n",
        "if st.button(\"Predict Difficulty\"):\n",
        "    if not description.strip():\n",
        "        st.warning(\"Please enter at least the problem description.\")\n",
        "    else:\n",
        "        combined_text = f\"{title} {description} {input_desc} {output_desc}\"\n",
        "        combined_text = preprocess_text(combined_text)\n",
        "\n",
        "        text_tfidf = tfidf.transform([combined_text])\n",
        "        text_len = scaler.transform([[len(combined_text)]])\n",
        "        X = hstack([text_tfidf, csr_matrix(text_len)])\n",
        "\n",
        "        class_pred = classifier.predict(X)[0]\n",
        "        score_pred = regressor.predict(X)[0]\n",
        "\n",
        "        class_map = {0: \"Hard\", 1: \"Medium\", 2: \"Easy\"}\n",
        "\n",
        "        st.success(\"Prediction Complete ‚úÖ\")\n",
        "        st.markdown(f\"### üß† Predicted Difficulty: **{class_map[class_pred]}**\")\n",
        "        st.markdown(f\"### ‚≠ê Predicted Score: **{score_pred:.2f} / 10**\")\n"
      ],
      "metadata": {
        "id": "9uzGInDXy7eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vA7qo_V324x7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}